import time
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager

KEYWORD = "Python"
CITIES = {
    "åŒ—äº¬": "530",
    "ä¸Šæµ·": "538",
    "å¹¿å·": "763",
    "æ·±åœ³": "765"
}
MAX_PAGE = 10  # æ¯ä¸ªåŸå¸‚æŠ“å–é¡µæ•°

def crawl_city_jobs(city_name, city_code, driver):
    print(f"ğŸ“ æŠ“å–åŸå¸‚ï¼š{city_name}")
    jobs = []

    for page in range(1, MAX_PAGE + 1):
        url = f"https://sou.zhaopin.com/?p={page}&jl={city_code}&kw={KEYWORD}"
        driver.get(url)
        time.sleep(3)

        job_cards = driver.find_elements(By.CSS_SELECTOR, ".joblist-box__item")

        if not job_cards:
            print("  âš ï¸ æœªæ‰¾åˆ°èŒä½å¡ç‰‡ï¼Œå¯èƒ½åˆ°è¾¾æœ«é¡µ")
            break

        for card in job_cards:
            try:
                title = card.find_element(By.CSS_SELECTOR, ".jobinfo__name").text
                salary = card.find_element(By.CSS_SELECTOR, ".jobinfo__salary").text
                company = card.find_element(By.CSS_SELECTOR, ".companyinfo__name").text
                detail_url = card.find_element(By.CSS_SELECTOR, "a").get_attribute("href")

                # èŒä½æ ‡ç­¾
                tags = [tag.text for tag in card.find_elements(By.CSS_SELECTOR, ".jobinfo__tag .joblist-box__item-tag")]

                # ç»éªŒå’Œå­¦å†
                exp = ""
                edu = ""
                for info in card.find_elements(By.CSS_SELECTOR, ".jobinfo__other-info-item"):
                    text = info.text.strip()
                    if "å¹´" in text or "ç»éªŒ" in text:
                        exp = text
                    elif "æœ¬ç§‘" in text or "å¤§ä¸“" in text or "å­¦å†" in text:
                        edu = text

                # æ‰“å¼€èŒä½è¯¦æƒ…é¡µè·å–èŒä½æè¿°
                driver.execute_script("window.open('');")
                driver.switch_to.window(driver.window_handles[1])
                driver.get(detail_url)
                time.sleep(2)

                try:
                    job_desc = driver.find_element(By.CLASS_NAME, "describtion__detail-content").text
                except:
                    job_desc = ""

                driver.close()
                driver.switch_to.window(driver.window_handles[0])

                jobs.append({
                    "å²—ä½åç§°": title,
                    "å…¬å¸åç§°": company,
                    "åŸå¸‚": city_name,
                    "è–ªèµ„": salary,
                    "ç»éªŒè¦æ±‚": exp,
                    "å­¦å†è¦æ±‚": edu,
                    "èŒä½æ ‡ç­¾": ", ".join(tags),
                    "èŒä½è¦æ±‚": job_desc  # âœ… æ–°å¢å­—æ®µ
                })

            except Exception as e:
                print(f"  âš ï¸ èŒä½è§£æå¤±è´¥ï¼Œè·³è¿‡ï¼š{e}")
                continue

    return jobs

def run_spider():
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
    driver.maximize_window()

    all_jobs = []
    for name, code in CITIES.items():
        city_jobs = crawl_city_jobs(name, code, driver)
        all_jobs.extend(city_jobs)

    driver.quit()

    with open("jobs_raw.json", "w", encoding="utf-8") as f:
        json.dump(all_jobs, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å…±æŠ“å– {len(all_jobs)} æ¡å²—ä½æ•°æ®ï¼Œå·²ä¿å­˜ä¸º jobs_raw.json")

if __name__ == "__main__":
    run_spider()

