import time
import json
import os
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException
from process_data import extract_empty_file_map
import argparse


def login(driver):
    """æ‰§è¡Œç™»å½•æ“ä½œ"""
    driver.get("https://passport.zhaopin.com/login?")
    time.sleep(3)
    telInput = driver.find_element(By.ID, "register-sms-1_input_1_phone")
    telInput.send_keys("18290207267")
    getCodeBtn = driver.find_element(By.CLASS_NAME, "zppp-sms__send")
    getCodeBtn.click()
    codeInput = driver.find_element(By.ID, "register-sms-1_input_2_validate_code")
    time.sleep(5)
    code = input("è¯·è¾“å…¥éªŒè¯ç ï¼š")
    codeInput.send_keys(code)
    checkbox = driver.find_element(By.XPATH, "//input[@id='accept']")
    checkbox.click()
    loginBtn = driver.find_element(By.CLASS_NAME, "zppp-submit")
    loginBtn.click()
    time.sleep(3)
    print("ç™»å½•æˆåŠŸ")

def extract_major_info(driver):
    """æå–ä¸“ä¸šä¿¡æ¯å¹¶å¯¼å‡ºä¸º JSON"""
    major_info = {}

    # ç‚¹å‡»â€œæ›´å¤šä¸“ä¸šâ€æŒ‰é’®
    more_major_btn = WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CSS_SELECTOR, ".expand-filter__more span"))
    )
    more_major_btn.click()
    time.sleep(1)

    # æå–ä¸€çº§å­¦ç§‘
    first_level_options = WebDriverWait(driver, 10).until(
        EC.presence_of_all_elements_located((By.CSS_SELECTOR, ".s-cascader__first-level .s-cascader__option"))
    )
    for first_level_option in first_level_options:
        first_level_name = first_level_option.find_element(By.CLASS_NAME, "s-cascader__option-content").text
        major_info[first_level_name] = {}

        # å±•å¼€ä¸€çº§å­¦ç§‘
        first_level_option.click()
        time.sleep(1)

        # æå–äºŒçº§å­¦ç§‘
        second_level_options = driver.find_elements(By.CSS_SELECTOR, ".s-cascader__second-level .s-cascader__option")
        for second_level_option in second_level_options:
            second_level_name = second_level_option.find_element(By.CLASS_NAME, "s-cascader__option-content").text
            major_info[first_level_name][second_level_name] = []

            # å±•å¼€äºŒçº§å­¦ç§‘
            second_level_option.click()
            time.sleep(1)

            # æå–å…·ä½“ä¸“ä¸š
            third_level_options = driver.find_elements(By.CSS_SELECTOR, ".s-cascader__third-level .s-cascader__select-option")
            for third_level_option in third_level_options:
                third_level_name = third_level_option.find_element(By.CLASS_NAME, "s-cascader__select-option-content").text
                major_info[first_level_name][second_level_name].append(third_level_name)
                # æ¯ä¸ªä¸“ä¸šå­˜ä¸€æ¬¡ JSON æ–‡ä»¶
                with open('major_info.json', 'w', encoding='utf-8') as f:
                    json.dump(major_info, f, ensure_ascii=False, indent=4)
                print(f"å·²ä¿å­˜ä¸“ä¸šï¼š{third_level_name}")

            # å…³é—­äºŒçº§å­¦ç§‘é¢æ¿
            second_level_option.click()
            time.sleep(1)

        # å…³é—­ä¸€çº§å­¦ç§‘é¢æ¿
        first_level_option.click()
        time.sleep(1)

    print("ä¸“ä¸šä¿¡æ¯å·²æˆåŠŸå¯¼å‡ºä¸º major_info.json")
    return major_info


def auto_login(driver):
    driver.get("https://passport.zhaopin.com/additional?appID=8b25de552a844b6c8493333ce98b9caf&redirectURL=https%3A%2F%2Fxiaoyuan.zhaopin.com%2Fredirect%3Furl%3Dhttps%253A%252F%252Fxiaoyuan.zhaopin.com%252Fsearch%252Fjn%253D2%253Fcity%253D538%2526cateType%253Dmajor")
    time.sleep(3)
    # åˆ‡æ¢åˆ°çŸ­ä¿¡ç™»å½•
    # ç­‰å¾…äºŒç»´ç ç™»å½•åŒºåŸŸå…ƒç´ åŠ è½½å®Œæˆ
    qrcode_login_element = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, 'zppp-panel-qrcode-bar__img'))
        )
        # ç‚¹å‡»äºŒç»´ç ç™»å½•åŒºåŸŸ
    qrcode_login_element.click()
    # åˆ‡æ¢åˆ°è´¦å·å¯†ç ç™»å½•
     # åˆ‡æ¢åˆ°â€œè´¦å¯†ç™»å½•â€æ ‡ç­¾
    tab_login_btn = WebDriverWait(driver, 10).until(
            EC.element_to_be_clickable((By.XPATH, "//li[text()='è´¦å¯†ç™»å½•']"))
        )
    tab_login_btn.click()
    username_input = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, "//input[@placeholder='ç”¨æˆ·å/æ‰‹æœºå·/é‚®ç®±']"))
        )
    username_input.send_keys("18290207267")

        # è¾“å…¥å¯†ç 
    password_input = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.XPATH, "//input[@placeholder='å¯†ç ']"))
        )
    password_input.send_keys("xgh123456")


    # å‹¾é€‰åŒæ„åè®®
    checkbox = driver.find_element(By.XPATH, "//input[@id='accept']")
    checkbox.click()

    # ç‚¹å‡»ç™»å½•æŒ‰é’®
    login_btn = WebDriverWait(driver, 10).until(
        EC.element_to_be_clickable((By.CLASS_NAME, "zppp-submit"))
    )
    login_btn.click()
    time.sleep(3)

    print("ç™»å½•æˆåŠŸ")

# é¡µé¢é€€å‡ºç™»å½•ä¼šå‡ºç°ç™»å½•
def relogin(driver):
    """é‡æ–°ç™»å½•"""
    try:
        login_btn = WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.CLASS_NAME, 'main-body__login-guide__button'))
        )
        # è·³è½¬åˆ°ç™»å½•é¡µé¢
        login_btn.click()
        auto_login(driver)
    except:
        print("é‡ç™»é™†å¼‚å¸¸ï¼Œè¯·äººå·¥æ£€æŸ¥")

def get_job_class_map():
    """è·å–ä¸“ä¸šä»£ç æ˜ å°„"""
    if os.path.exists('./crawler/persist/job_class_map_v2.json'):
        with open('./crawler/persist/job_class_map_v2.json', 'r', encoding='utf-8') as f:
            job_class_map = json.load(f)
        return job_class_map
    else:
        return {}
    

def get_base_info(item,start_index):
    """è·å–èŒä½åŸºç¡€ä¿¡æ¯"""
    position_card = item
    return {
        "èŒä½ID": start_index,
        "èŒä½åç§°": position_card.find_element(By.CLASS_NAME, "position-card__job-name").text,
        "åŸå¸‚": position_card.find_element(By.CLASS_NAME, "position-card__city-name").text,
        "è–ªèµ„": position_card.find_element(By.CLASS_NAME, "position-card__salary").text,
        "å…¬å¸åç§°": position_card.find_element(By.CLASS_NAME, "position-card__company__name").text,
        "å…¬å¸æ ‡ç­¾": [tag.text for tag in position_card.find_elements(By.CLASS_NAME, "position-card__company__tabs-item")],
        "å­¦å†è¦æ±‚": position_card.find_elements(By.CLASS_NAME, "position-card__tags__item")[1].text,
        "æ˜¯å¦éœ€è¦å·¥ä½œç»éªŒ": position_card.find_elements(By.CLASS_NAME, "position-card__tags__item")[2].text
    }

def get_additional_info(driver):
    """è·å–èŒä½è¡¥å……ä¿¡æ¯"""
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CLASS_NAME, "job-banner"))
    )
    
    try:
        people_count = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, ".job-banner__desc-text:last-child"))
        ).text
    except TimeoutException:
        people_count = "ä¿¡æ¯æš‚æ— "

    try:
        preferred_majors = [span.text for span in WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CLASS_NAME, "job-info__major-tags"))
        ).find_elements(By.TAG_NAME, "span")]
    except TimeoutException:
        preferred_majors = ["ä¿¡æ¯æš‚æ— "]

    return {
        "æ‹›è˜äººæ•°": people_count,
        "ä¼˜é€‰ä¸“ä¸š": ", ".join(preferred_majors)
    }



def save_incrementally(file_path, new_data):
    """å¢é‡ä¿å­˜æ•°æ®"""
    # è¯»å–ç°æœ‰æ•°æ®
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            existing = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        existing = []
    
    # æ·»åŠ æ–°æ•°æ®
    existing.append(new_data)
    # å†™å…¥æ–‡ä»¶
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(existing, f, ensure_ascii=False, indent=2)


def check_login_status(driver):
    """
    æ£€æŸ¥æ˜¯å¦ç™»å½•äº†æ™ºè”æ‹›è˜ç½‘ç«™ã€‚
    
    å‚æ•°:
        driver (webdriver): Selenium WebDriver å®ä¾‹ã€‚
    
    è¿”å›:
        bool: å¦‚æœå·²ç™»å½•ï¼Œè¿”å› Trueï¼›å¦åˆ™è¿”å› Falseã€‚
    """
    try:
        # æ‰¾æœ‰æ²¡æœ‰è¿™ä¸ªå…ƒç´ ï¼Œå¦‚æœæœ‰ï¼Œè¯´æ˜é€€å‡ºç™»å½•äº†
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CLASS_NAME, 'main-body__login-guide'))
        )

        return False
    except:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç™»å½•å¼•å¯¼å…ƒç´ ï¼Œåˆ™è¯´æ˜å·²ç™»å½•
        return True




def crawl_major_jobs(driver, major_name, major_code,start_index):
    """è¯»å–ä¸“ä¸šçš„èŒä½ä¿¡æ¯ï¼ˆä¿®æ”¹ç‰ˆï¼‰"""
    print(f"ğŸ“Œ æ­£åœ¨æŠ“å– {major_name} ç›¸å…³èŒä½...")
    # åˆå§‹åŒ–èŒä½id
    start_ID = 0
    page_index = 0
    try:
        # åˆå§‹åŒ–æ•°æ®æ–‡ä»¶
        data_file = f"./crawler/data/{start_index}_{major_code}_{major_name}.json"
        os.makedirs(os.path.dirname(data_file), exist_ok=True)
        if not os.path.exists(data_file):
            with open(data_file, "w", encoding="utf-8") as f:
                json.dump([], f, ensure_ascii=False, indent=4)
        time.sleep(3)# æ–°å»ºæ–‡ä»¶ç­‰1s
        url = f"https://xiaoyuan.zhaopin.com/search/index?refcode=4404&cateType=major&city=538%2C539%2C540&degree=4%2C3%2C10%2C1&sourceType=2&position=2%2C5&major={major_code}"
        driver.get(url)
        time.sleep(5)
        
        # ç­‰å¾…ä¸»è¦å†…å®¹åŠ è½½,ç›´æ¥è·å–ä½ æƒ³è¦çš„ä¿¡æ¯å¡ç‰‡ï¼Œåˆ«è½¦å“ªäº›ä¸ªæ›´å¤§çš„listä¹‹ç±»çš„ï¼Œå¦åˆ™ä¼šå› ä¸ºåˆ·æ–°å¯¼è‡´å®šä½å¤±è´¥
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.CLASS_NAME, "position-card"))
        )
        time.sleep(2) # ç­‰å¾…é¡µé¢åŠ è½½å®Œæ¯•
        while True:
            # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†æœ¬é¡µæ‰€æœ‰èŒä½åŸºç¡€ä¿¡æ¯
            page_index += 1
            current_time=time.time()
            # æ‹¼æ¥ä¸€æ¡logå­—ç¬¦ä¸²
            # æ—¶é—´å­—ç¬¦ä¸²è½¬åŒ–ä¸º
            time_str = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime(current_time))
            log_str = f"ã€{time_str}ã€‘ã€{major_name}ã€‘ç¬¬ {page_index} é¡µæ•°æ®è·å–å¼€å§‹ï¼"
            save_log(log_str)
            position_items = driver.find_elements(By.CLASS_NAME, "position-card")
            page_data = []
            position_cards = []
            # è·å–æ‰€æœ‰çš„å¡ç‰‡ä¿¡æ¯ï¼Œä¿å­˜åˆ°position_cardsåˆ—è¡¨ä¸­
            for item in position_items:
                try:
                    position_card = item
                    position_cards.append(position_card)
                except Exception as e:
                    print(f"è·å–èŒä½å¡ç‰‡å¤±è´¥ï¼š{str(e)}")
                    continue
            # è·å–æ‰€æœ‰åŸºç¡€ä¿¡æ¯
            for item in position_cards:
                try:
                    start_ID += 1
                    base_info = get_base_info(item,start_ID)
                    page_data.append(base_info)
                    print(f"ã€{major_name}ã€‘ç¬¬ {start_ID} ä¸ªèŒä½ï¼šğŸ” å·²è·å– {base_info['èŒä½åç§°']} çš„åŸºç¡€ä¿¡æ¯")
                except Exception as e:
                    print(f"è·å–åŸºç¡€ä¿¡æ¯å¤±è´¥ï¼š{str(e)}")
                    continue

            # ç¬¬äºŒé˜¶æ®µï¼šé€ä¸ªè·å–è¯¦ç»†ä¿¡æ¯
            original_window = driver.current_window_handle
            index = 0
            while index < (len(position_items)):
                try:
                    # é‡æ–°å®šä½å…ƒç´ é˜²æ­¢å¤±æ•ˆ
                    item = driver.find_elements(By.CLASS_NAME, "position-list__item")[index]
                    position_card = item.find_element(By.CLASS_NAME, "position-card")
                    
                    # åœ¨æ–°æ ‡ç­¾é¡µæ‰“å¼€è¯¦æƒ…
                    position_card.click()
                    WebDriverWait(driver, 10).until(EC.number_of_windows_to_be(2))
                    driver.switch_to.window(driver.window_handles[-1])
                    
                    # è·å–è¡¥å……ä¿¡æ¯
                    additional_info = get_additional_info(driver)
                    page_data[index].update(additional_info)
                    
                    # å…³é—­è¯¦æƒ…é¡µ
                    driver.close()
                    driver.switch_to.window(original_window)
                    
                    # å®æ—¶ä¿å­˜æ•°æ®
                    save_incrementally(data_file, page_data[index])
                    print(f"âœ… å·²ä¿å­˜ {page_data[index]['èŒä½åç§°']} çš„å®Œæ•´ä¿¡æ¯")
                    index += 1
                    
                except Exception as e:
                    print(f"å¤„ç†ç¬¬ {index+1} ä¸ªèŒä½æ—¶å‡ºé”™ï¼š{str(e)}")
                    # æ¢å¤é¡µé¢çŠ¶æ€
                    while len(driver.window_handles) > 1:
                        driver.switch_to.window(driver.window_handles[-1])
                        driver.close()
                    driver.switch_to.window(original_window)
                    driver.refresh()
                    WebDriverWait(driver, 15).until(
                        EC.presence_of_element_located((By.CLASS_NAME, "position-list")))
                    # input("è¯·æ‰‹åŠ¨å¤„ç†è¯¥èŒä½ï¼ŒæŒ‰å›è½¦ç»§ç»­")
                    continue
                    # return False

            # ç¿»é¡µæ“ä½œ
            try:
                next_btn = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, ".pagination__arrow-next:not(.pagination__arrow-next--disabled)")))
                next_btn.click()
                
                # ç­‰å¾…æ–°é¡µé¢åŠ è½½
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.CLASS_NAME, "position-list"))
                )
                if not check_login_status(driver):
                    return False
                time.sleep(2)
            except Exception:
                print("â¹ å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                if not check_login_status(driver):
                    return False
                else:
                    return True
                  
    except Exception as e:
        print(f"â¸ {major_name} ä¸­æ–­äºç¬¬{page_index}é¡µï¼š{str(e)}")
        return False

def get_job_positions(driver, job_class_map,args_auto):
    """è·å–èŒä½ä¿¡æ¯ï¼ˆæœ‰åºmapç‰ˆæœ¬ï¼‰"""
    auto_login(driver)
    print("âœ… ç™»å½•æˆåŠŸï¼Œå¼€å§‹çˆ¬å–æ•°æ®")
    auto_mode = args_auto

    # è½¬æ¢ä¸ºæœ‰åºåˆ—è¡¨ï¼ˆä¿æŒåŸé¡ºåºï¼‰
    major_list = list(job_class_map.items())
    
    # åŠ è½½ä¸Šæ¬¡è¿›åº¦
    start_index = 0
    if os.path.exists('./crawler/persist/progress.json'):
        with open('./crawler/persist/progress.json', 'r', encoding='utf-8') as f:
            progress = json.load(f)
            last_major = progress.get('last_major')
            
            # åœ¨åŸå§‹åˆ—è¡¨ä¸­æŸ¥æ‰¾ç´¢å¼•
            for idx, (name, _) in enumerate(major_list):
                if name == last_major:
                    start_index = idx + 1  # ä»ä¸‹ä¸€ä¸ªä¸“ä¸šå¼€å§‹
                    break
    idx = start_index #å½“å‰çš„ä¸“ä¸šç´¢å¼•               
    # éå†ä¸“ä¸šåˆ—è¡¨
    while idx < len(major_list) and idx >= start_index:
        major_name, major_code = major_list[idx] 
        try:
            print(f"\nğŸ” æ­£åœ¨å¤„ç†ä¸“ä¸š ({idx+1}/{len(major_list)})ï¼š{major_name}")
            
            # æ‰§è¡Œçˆ¬å–ï¼ˆæ–°å¢å¼‚å¸¸æ•è·ï¼‰
            success = crawl_major_jobs(driver, major_name, major_code,idx+1) # ä¿®æ”¹ä¸ºä¾¿äºè®¡æ•°çš„ä»1å¼€å§‹çš„ç´¢å¼•ï¼Œæ‰€ä»¥è¿™é‡Œè¦+1
            
            if not success:
                print(f"â¸ {major_name} çˆ¬å–æœªå®Œæˆï¼Œå¯èƒ½æ˜¯é€€å‡ºç™»å½•äº†")
                # é‡æ–°ç™»å½•
                auto_login(driver)
                print("ğŸ” é‡æ–°ç™»å½•æˆåŠŸï¼Œç»§ç»­çˆ¬å–")
            else:
                print(f"âœ… {major_name} çˆ¬å–å®Œæˆ")
                idx += 1
                # ä¼‘æ¯ä¸€ä¼šæ¥ç€çˆ¬
                time.sleep(3)
            save_progress({
                "last_major": major_name,
                "processed_index": idx,
                "total_count": len(major_list),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })
            
            if not auto_mode:
                print("ğŸš§ å·²è¿›å…¥äººå·¥æ¨¡å¼ï¼Œå°†ä¾é è¾“å…¥çš„ä¸“ä¸šç è¯»å–å¯¹åº”çš„ä¸“ä¸š")
                cmd = input("è¯·è¾“å…¥ä¸‹ä¸€ä¸ªçˆ¬å–ä¸“ä¸šçš„ä»£ç ï¼ˆè¾“å…¥ -1é€€å‡ºï¼‰ï¼š")
                if cmd == "-1":
                    exit(0)
                else:
                    # è½¬ä¸ºæ•°å­—
                    major_code = int(cmd)
                    # åœ¨majar_listä¸­æŸ¥æ‰¾å¯¹åº”çš„ä¸“ä¸š
                    for i,(name, code) in enumerate(major_list):
                        if code == major_code:
                            idx = i
                            break
                    else:
                        print(f"âŒ æœªæ‰¾åˆ°ä¸“ä¸šä»£ç  {major_code}ï¼Œè¯·æ£€æŸ¥è¾“å…¥æ˜¯å¦æ­£ç¡®")
                        exit(1)
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·ä¸»åŠ¨ä¸­æ–­ï¼Œä¿å­˜å½“å‰è¿›åº¦")
            save_progress({
                "last_major": major_list[idx-1][0] if idx >0 else "",
                "processed_index": idx-1,
                "total_count": len(major_list),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })
            break
            
        except Exception as e:
            print(f"âŒ ä¸¥é‡é”™è¯¯ï¼š{str(e)}")
            save_progress({
                "last_major": major_list[idx-1][0] if idx >0 else "",
                "processed_index": idx-1,
                "total_count": len(major_list),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })
            raise
def save_log(log_data):
    """å¢å¼ºå‹æ—¥å¿—ä¿å­˜"""
    # åˆ›å»ºä¸€ä¸ªæ—¥å¿—æ–‡ä»¶txt,é€æ¡appendæ—¥å¿—è®°å½•
    log_file = "./crawler/persist/log.txt"
    try:
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(log_data)
            f.write("\n")
    except Exception as e:
        print(f"âš ï¸ æ—¥å¿—ä¿å­˜å¤±è´¥ï¼š{str(e)}")
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] æ—¥å¿—ä¿å­˜å¤±è´¥ï¼š{str(e)}")
def save_progress(progress_data):
    """å¢å¼ºå‹è¿›åº¦ä¿å­˜"""
    progress_data.update({
        "version": "1.1",
        "status": "interrupted" if progress_data["processed_index"] < progress_data["total_count"]-1 else "completed"
    })
    # 
    try:
        with open('./crawler/persist/progress.json', 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜ï¼š{progress_data['last_major']} ({progress_data['status']})")
    except Exception as e:
        print(f"âš ï¸ è¿›åº¦ä¿å­˜å¤±è´¥ï¼š{str(e)}")
        # å°è¯•å¤‡ä»½ä¿å­˜
        with open('./crawler/persist/progress_backup.json', 'w') as f:
            json.dump(progress_data, f)


   


def run_spider(args):
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œçˆ¬è™«"""
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
    driver.maximize_window()

    try:
        # è·å–ä¸“ä¸šä»£ç æ˜ å°„
        job_class_map = get_job_class_map()

        get_job_positions(driver, job_class_map, args.auto)

        print("çˆ¬å–å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜åˆ° job_positions.json")
    finally:
        driver.quit()


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument("--auto", action="store_true", default=False, help="é»˜è®¤éè‡ªåŠ¨æ¨¡å¼ï¼Œæ‰‹åŠ¨è¾“å…¥ä¸“ä¸šä»£ç ")
    args = parser.parse_args()
    run_spider(args)