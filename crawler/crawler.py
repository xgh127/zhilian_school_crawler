import time
import json
import os
import pandas as pd
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.support.ui import WebDriverWait
from selenium.common.exceptions import TimeoutException
from process_data import extract_empty_file_map


def login(driver):
    """æ‰§è¡Œç™»å½•æ“ä½œ"""
    driver.get("https://passport.zhaopin.com/login?")
    time.sleep(3)
    telInput = driver.find_element(By.ID, "register-sms-1_input_1_phone")
    telInput.send_keys("18290207267")
    getCodeBtn = driver.find_element(By.CLASS_NAME, "zppp-sms__send")
    getCodeBtn.click()
    codeInput = driver.find_element(By.ID, "register-sms-1_input_2_validate_code")
    time.sleep(5)
    code = input("è¯·è¾“å…¥éªŒè¯ç ï¼š")
    codeInput.send_keys(code)
    checkbox = driver.find_element(By.XPATH, "//input[@id='accept']")
    checkbox.click()
    loginBtn = driver.find_element(By.CLASS_NAME, "zppp-submit")
    loginBtn.click()
    time.sleep(3)
    print("ç™»å½•æˆåŠŸ")

def get_job_class_map():
    """è·å–ä¸“ä¸šä»£ç æ˜ å°„"""
    if os.path.exists('./crawler/persist/job_class_map_new.json'):
        with open('./crawler/persist/job_class_map_new.json', 'r', encoding='utf-8') as f:
            job_class_map = json.load(f)
        return job_class_map
    else:
        return {}
    

def get_base_info(item,start_index):
    """è·å–èŒä½åŸºç¡€ä¿¡æ¯"""
    position_card = item
    return {
        "èŒä½ID": start_index,
        "èŒä½åç§°": position_card.find_element(By.CLASS_NAME, "position-card__job-name").text,
        "åŸå¸‚": position_card.find_element(By.CLASS_NAME, "position-card__city-name").text,
        "è–ªèµ„": position_card.find_element(By.CLASS_NAME, "position-card__salary").text,
        "å…¬å¸åç§°": position_card.find_element(By.CLASS_NAME, "position-card__company__name").text,
        "å…¬å¸æ ‡ç­¾": [tag.text for tag in position_card.find_elements(By.CLASS_NAME, "position-card__company__tabs-item")],
        "å­¦å†è¦æ±‚": position_card.find_elements(By.CLASS_NAME, "position-card__tags__item")[1].text,
        "æ˜¯å¦éœ€è¦å·¥ä½œç»éªŒ": position_card.find_elements(By.CLASS_NAME, "position-card__tags__item")[2].text
    }

def get_additional_info(driver):
    """è·å–èŒä½è¡¥å……ä¿¡æ¯"""
    WebDriverWait(driver, 10).until(
        EC.presence_of_element_located((By.CLASS_NAME, "job-banner"))
    )
    
    try:
        people_count = WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, ".job-banner__desc-text:last-child"))
        ).text
    except TimeoutException:
        people_count = "ä¿¡æ¯æš‚æ— "

    try:
        preferred_majors = [span.text for span in WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.CLASS_NAME, "job-info__major-tags"))
        ).find_elements(By.TAG_NAME, "span")]
    except TimeoutException:
        preferred_majors = ["ä¿¡æ¯æš‚æ— "]

    return {
        "æ‹›è˜äººæ•°": people_count,
        "ä¼˜é€‰ä¸“ä¸š": ", ".join(preferred_majors)
    }



def save_incrementally(file_path, new_data):
    """å¢é‡ä¿å­˜æ•°æ®"""
    # è¯»å–ç°æœ‰æ•°æ®
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            existing = json.load(f)
    except (FileNotFoundError, json.JSONDecodeError):
        existing = []
    
    # æ·»åŠ æ–°æ•°æ®
    existing.append(new_data)
    # å†™å…¥æ–‡ä»¶
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(existing, f, ensure_ascii=False, indent=2)




def crawl_major_jobs(driver, major_name, major_code,start_index):
    """è¯»å–ä¸“ä¸šçš„èŒä½ä¿¡æ¯ï¼ˆä¿®æ”¹ç‰ˆï¼‰"""
    print(f"ğŸ“Œ æ­£åœ¨æŠ“å– {major_name} ç›¸å…³èŒä½...")
    # åˆå§‹åŒ–èŒä½id
    start_ID = 0
    try:
        # åˆå§‹åŒ–æ•°æ®æ–‡ä»¶
        data_file = f"./crawler/data/{start_index}_{major_code}_{major_name}.json"
        os.makedirs(os.path.dirname(data_file), exist_ok=True)
        if not os.path.exists(data_file):
            with open(data_file, "w", encoding="utf-8") as f:
                json.dump([], f, ensure_ascii=False, indent=4)

        url = f"https://xiaoyuan.zhaopin.com/search/index?refcode=4404&cateType=major&city=538%2C539%2C540&degree=4%2C3%2C10%2C1&sourceType=2&position=2%2C5&major={major_code}"
        driver.get(url)
        
        # ç­‰å¾…ä¸»è¦å†…å®¹åŠ è½½
        WebDriverWait(driver, 15).until(
            EC.presence_of_element_located((By.CLASS_NAME, "position-card"))
        )

        while True:
            # ç¬¬ä¸€é˜¶æ®µï¼šæ”¶é›†æœ¬é¡µæ‰€æœ‰èŒä½åŸºç¡€ä¿¡æ¯
            position_items = driver.find_elements(By.CLASS_NAME, "position-card")
            page_data = []
            position_cards = []
            # è·å–æ‰€æœ‰çš„å¡ç‰‡ä¿¡æ¯ï¼Œä¿å­˜åˆ°position_cardsåˆ—è¡¨ä¸­
            for item in position_items:
                try:
                    position_card = item
                    position_cards.append(position_card)
                except Exception as e:
                    print(f"è·å–èŒä½å¡ç‰‡å¤±è´¥ï¼š{str(e)}")
                    continue
            # è·å–æ‰€æœ‰åŸºç¡€ä¿¡æ¯
            for item in position_cards:
                try:
                    start_ID += 1
                    base_info = get_base_info(item,start_ID)
                    page_data.append(base_info)
                    print(f"ã€{major_name}ã€‘ç¬¬ {start_ID} ä¸ªèŒä½ï¼šğŸ” å·²è·å– {base_info['èŒä½åç§°']} çš„åŸºç¡€ä¿¡æ¯")
                except Exception as e:
                    print(f"è·å–åŸºç¡€ä¿¡æ¯å¤±è´¥ï¼š{str(e)}")
                    continue

            # ç¬¬äºŒé˜¶æ®µï¼šé€ä¸ªè·å–è¯¦ç»†ä¿¡æ¯
            original_window = driver.current_window_handle
            for index in range(len(position_items)):
                try:
                    # é‡æ–°å®šä½å…ƒç´ é˜²æ­¢å¤±æ•ˆ
                    item = driver.find_elements(By.CLASS_NAME, "position-list__item")[index]
                    position_card = item.find_element(By.CLASS_NAME, "position-card")
                    
                    # åœ¨æ–°æ ‡ç­¾é¡µæ‰“å¼€è¯¦æƒ…
                    position_card.click()
                    WebDriverWait(driver, 10).until(EC.number_of_windows_to_be(2))
                    driver.switch_to.window(driver.window_handles[-1])
                    
                    # è·å–è¡¥å……ä¿¡æ¯
                    additional_info = get_additional_info(driver)
                    page_data[index].update(additional_info)
                    
                    # å…³é—­è¯¦æƒ…é¡µ
                    driver.close()
                    driver.switch_to.window(original_window)
                    
                    # å®æ—¶ä¿å­˜æ•°æ®
                    save_incrementally(data_file, page_data[index])
                    print(f"âœ… å·²ä¿å­˜ {page_data[index]['èŒä½åç§°']} çš„å®Œæ•´ä¿¡æ¯")
                    
                except Exception as e:
                    print(f"å¤„ç†ç¬¬ {index+1} ä¸ªèŒä½æ—¶å‡ºé”™ï¼š{str(e)}")
                    # æ¢å¤é¡µé¢çŠ¶æ€
                    while len(driver.window_handles) > 1:
                        driver.switch_to.window(driver.window_handles[-1])
                        driver.close()
                    driver.switch_to.window(original_window)
                    driver.refresh()
                    WebDriverWait(driver, 15).until(
                        EC.presence_of_element_located((By.CLASS_NAME, "position-list")))
                    break

            # ç¿»é¡µæ“ä½œ
            try:
                next_btn = WebDriverWait(driver, 10).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, ".pagination__arrow-next:not(.pagination__arrow-next--disabled)")))
                next_btn.click()
                
                # ç­‰å¾…æ–°é¡µé¢åŠ è½½
                WebDriverWait(driver, 15).until(
                    EC.presence_of_element_located((By.CLASS_NAME, "position-list"))
                )
                time.sleep(2)
            except Exception:
                print("â¹ å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                break
        return True
    except Exception as e:
        print(f"â¸ {major_name} ä¸­æ–­äºï¼š{str(e)}")
        return False

def get_job_positions(driver, job_class_map):
    """è·å–èŒä½ä¿¡æ¯ï¼ˆæœ‰åºmapç‰ˆæœ¬ï¼‰"""
    login(driver)
    print("âœ… ç™»å½•æˆåŠŸï¼Œå¼€å§‹çˆ¬å–æ•°æ®")

    # è½¬æ¢ä¸ºæœ‰åºåˆ—è¡¨ï¼ˆä¿æŒåŸé¡ºåºï¼‰
    major_list = list(job_class_map.items())
    
    # åŠ è½½ä¸Šæ¬¡è¿›åº¦
    start_index = 0
    if os.path.exists('./crawler/persist/progress.json'):
        with open('./crawler/persist/progress.json', 'r', encoding='utf-8') as f:
            progress = json.load(f)
            last_major = progress.get('last_major')
            
            # åœ¨åŸå§‹åˆ—è¡¨ä¸­æŸ¥æ‰¾ç´¢å¼•
            for idx, (name, _) in enumerate(major_list):
                if name == last_major:
                    start_index = idx + 1  # ä»ä¸‹ä¸€ä¸ªä¸“ä¸šå¼€å§‹
                    break

    # éå†ä¸“ä¸šåˆ—è¡¨
    for idx in range(start_index, len(major_list)):
        major_name, major_code = major_list[idx]
        
        try:
            print(f"\nğŸ” æ­£åœ¨å¤„ç†ä¸“ä¸š ({idx+1}/{len(major_list)})ï¼š{major_name}")
            
            # æ‰§è¡Œçˆ¬å–ï¼ˆæ–°å¢å¼‚å¸¸æ•è·ï¼‰
            success = crawl_major_jobs(driver, major_name, major_code,idx+1)
            
            if not success:
                print(f"â¸ {major_name} çˆ¬å–æœªå®Œæˆï¼Œä¿ç•™è¿›åº¦")
                break
                
            # æ›´æ–°è¿›åº¦
            save_progress({
                "last_major": major_name,
                "processed_index": idx,
                "total_count": len(major_list),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })
            
        except KeyboardInterrupt:
            print("\nğŸ›‘ ç”¨æˆ·ä¸»åŠ¨ä¸­æ–­ï¼Œä¿å­˜å½“å‰è¿›åº¦")
            save_progress({
                "last_major": major_list[idx-1][0] if idx >0 else "",
                "processed_index": idx-1,
                "total_count": len(major_list),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })
            break
            
        except Exception as e:
            print(f"âŒ ä¸¥é‡é”™è¯¯ï¼š{str(e)}")
            save_progress({
                "last_major": major_list[idx-1][0] if idx >0 else "",
                "processed_index": idx-1,
                "total_count": len(major_list),
                "timestamp": time.strftime("%Y-%m-%d %H:%M:%S")
            })
            raise

def save_progress(progress_data):
    """å¢å¼ºå‹è¿›åº¦ä¿å­˜"""
    progress_data.update({
        "version": "1.1",
        "status": "interrupted" if progress_data["processed_index"] < progress_data["total_count"]-1 else "completed"
    })
    
    try:
        with open('./crawler/persist/progress.json', 'w', encoding='utf-8') as f:
            json.dump(progress_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ è¿›åº¦å·²ä¿å­˜ï¼š{progress_data['last_major']} ({progress_data['status']})")
    except Exception as e:
        print(f"âš ï¸ è¿›åº¦ä¿å­˜å¤±è´¥ï¼š{str(e)}")
        # å°è¯•å¤‡ä»½ä¿å­˜
        with open('./crawler/persist/progress_backup.json', 'w') as f:
            json.dump(progress_data, f)


   


def run_spider():
    """ä¸»å‡½æ•°ï¼Œè¿è¡Œçˆ¬è™«"""
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))
    driver.maximize_window()

    try:
        # è·å–ä¸“ä¸šä»£ç æ˜ å°„
        job_class_map = get_job_class_map()

        get_job_positions(driver, job_class_map)

        print("çˆ¬å–å®Œæˆï¼Œæ•°æ®å·²ä¿å­˜åˆ° job_positions.json")
    finally:
        driver.quit()


if __name__ == "__main__":
    run_spider()